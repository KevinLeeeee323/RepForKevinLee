## 决策树学习
- 非叶节点定义规则, 叶节点定义类别

### 信息熵
- 小概率事件信息量大, 事出反常必有妖-一旦发生，就会带来大量新信息
- 定义**信息量**:$I(a_i)=-\log_{2}{p(a_i)}$
  - 事件$a_i$概率越小, $I(a_i)$越大

- 信息熵
  - 事件 A 有 n 个取值, $A=a_1, a_2...a_n$, 则:
    $$
    H(A)=-\sum_{i=1}^{n}{[p(A=a_i) \cdot \log_{2}{p(A=a_i)]}}
    $$
    - 如何理解这一公式? 为什么还要乘 x_i?
  - 沿着树从根节点向叶节点走, 层次增加, 信息熵减少.决策树的叶节点, 发生概率$P=1$, 信息熵为 0

- 手动构建决策树
  - 完全以数据作为依据
  - 创建分叉直到当前分叉内的样本都有同样的输出类别标签, 提取其共同特点作为当前分叉的分支依据

- 通过信息熵构建决策树: ID3 算法(Iterative Dichotomiser, 迭代二分器)
  - 使用该算法构建决策树, 能够使信息熵**最速下降**
  - 思想方法
    - 某个属性$N_v$对于当前样本集合进行划分, 可以定义属性 $N_v$ 对分类结果($A=a_1, a_2, ...a_t$)的信息熵:
    $$
    I(N_v)=-\sum_{j=1}^{t}{p(A=a_j)\cdot log_2{p(A=a_j)}}
    $$
    - 信息增益(负熵):刻画当前属性对于当前样本集合的划分效果
  - PPT上是默认以 10 为底的对数, 有些地方以 2 为底
  - 进行迭代, 最终构建成树
  - 在 ID3 算法上的改进: C4.5算法
