## Q学习
### 什么是Q 学习?
> 基于状态, 动作, 奖励三个要素, 指导智能体在对应环境下做出适当动作反应

### 一个例子:走出房间
[例子](https://cloud.tencent.com/developer/article/2163196)
- 任务目标:处于任何一个房间的机器人都能够找到一条最优路径走到室外
- 通过图论建模该问题
- 每条边的权重 相当于 一个房间走到另一个房间的奖励. 由于目标是走出房间, 因此走出房间的边奖励值更大(100), 没走出房间的边奖励值为 0
- 将边的权值通过邻接矩阵的方式存储

### 操作
- 需要先设定执行动作的规则
  - 假设规则是每次选择奖励最大的边(贪心), 但可能会在某些点困住, 因为有些节点边少, 且存在的边数值都是 0

- 因此, 基于强化学习, 应需要对应修改**行为规则**
  - 设当前这一步, 从A 到 B 房间
  - 对于走到更有可能走出房间的房间 B, 应该增加A 到 B的奖励
  - 也即, 考虑当下动作所带来的未来所带来的回报(不管是正增益 or 负增益
  - ![pic1](Qlearn.png)
  - 具体算法 ![pic2](Qlearn_algo.png)



## 决策树学习
- 非叶节点定义规则, 叶节点定义类别

### 信息熵
- 小概率事件信息量大, 事出反常必有妖-一旦发生，就会带来大量新信息
- 定义**信息量**:$I(a_i)=-\log_{2}{p(a_i)}$
  - 事件$a_i$概率越小, $I(a_i)$越大

- 信息熵
  - 事件 A 有 n 个取值, $A=a_1, a_2...a_n$, 则:
    $$
    H(A)=-\sum_{i=1}^{n}{[p(A=a_i) \cdot \log_{2}{p(A=a_i)]}}
    $$
    - 如何理解这一公式? 为什么还要乘 x_i?
  - 沿着树从根节点向叶节点走, 层次增加, 信息熵减少.决策树的叶节点, 发生概率$P=1$, 信息熵为 0

- 手动构建决策树
  - 完全以数据作为依据
  - 创建分叉直到当前分叉内的样本都有同样的输出类别标签, 提取其共同特点作为当前分叉的分支依据

- 通过信息熵构建决策树: ID3 算法(Iterative Dichotomiser, 迭代二分器)
  - 使用该算法构建决策树, 能够使信息熵**最速下降**
  - 思想方法
    - 某个属性$N_v$对于当前样本集合进行划分, 可以定义属性 $N_v$ 对分类结果($A=a_1, a_2, ...a_t$)的信息熵:
    $$
    I(N_v)=-\sum_{j=1}^{t}{p(A=a_j)\cdot log_2{p(A=a_j)}}
    $$
    - 信息增益(负熵):刻画当前属性对于当前样本集合的划分效果
  - PPT上是默认以 10 为底的对数, 有些地方以 2 为底
  - 进行迭代, 最终构建成树
  - 在 ID3 算法上的改进: C4.5算法


  